---
layout: post
title:  大数据与人工智能培训-机器学习及其典型算法
date:   2019-05-31
excerpt: 兰智大数据与人工智能培训第二天下午的内容
tag:
- Python
- AI
- maths
- algorithm
comments: true
---

### 什么是机器学习?  
给定<strong>数据</strong>(样本, 实例)和一定的<strong>学习规则</strong>, 从数据中<strong>获取知识</strong>的能力.  
e.g.:  
	- 输入数据(行业+工作年限)  
	- 模型(决策树) -- 科学家的研究成果, 大量的论文都是做这个事的  
	- 获取知识(高收入, 中等收入, 低收入) -- 机器学习的预测结果    
	
### 机器学习的关键问题     
<strong>机器学习 = 模型(假设) + 评价 + 优化方法</strong>  
- 模型函数    
	$y(x,w)=w_0+w_1w+w_2x^2+...+w_Mx^M=\sum\limits_{j=0}^Mw_jx^j$   
- 评价     
	$E(w)=\frac{1}{2}\sum\limits_{n=1}^N(y(x_n,w)-t_n)^2$   
- 优化方法    
	代价函数最小化(梯度下降等)    

1. 模型(以监督学习为例)    
	- 数据集包括: 输入空间$X=\lbrace x_1,x_2,...,x_n\rbrace$和输出空间$Y=\lbrace y_1,y_2,...,y_n\rbrace$    
	- 模型是一系列能建模数据集的决策函数, 构成假设空间. 通俗地说, 算法结构+优化策略.    
2. 策略    
- 损失函数    
	对于给定的输入和假设空间中选择的决策函数模型, 由给出相应的输入, 这个输出的预测值与真实值可能一致, 也可能不一致, 用一个损失函数或代价函数来度量预测的错误程度. <strong>损失函数值越小, 模型就越好!</strong>    
	平方损失函数: $L(y,\hat{y})=(y-f(x,\theta))^2$
	负对数损失函数: $L(y,f(x,\theta)=-\sum\limits_{i=1}^Cy_ilogf_i(x,\theta))$
	
- 期望风险: 期望风险是全局的概念, 对于所有样本(包含待预测的样本)的琐事     
- 经验风险: 经验风险是局部概念, 对训练集中的所有样本点损失函数的平均最小化    
- 泛化误差: 反应模型对位置数据的预测能力, 可表示:    
	|期望风险-经验风险|   
	
- 机器学习  
<strong>从有限的观测数据样本中</strong>学习出具有一般性的, 可泛化推广的规律, 并可以将总结出来的规律推广应用到<strong>未观测样本<strong>的预测上.     

- 经验误差并不是越小越好, 因为过小的误差会导致模型出现<strong>过拟合</strong>.      

- 过拟合和欠拟合: 都可以导致很差的模型表现. 但是, 目前大部分机器学习实际应用时遇到的问题都是过拟合.     
	限制过拟合的方法    
	提前终止|当验证集上的效果变差的时候   
	扩增数据集|有时候往往拥有更多的数据胜过一个好的模型   
	正则化|在目标函数或代价函数后面加上一个正则项()    
	
3. 优化算法   
梯度下降    
- 随机梯度下降(SGD): 更新每一参数时, <strong>使用一个样本</strong>来进行更新(优点: 训练速度快; 缺点: 不是全局最优)    
- 批量梯度下降(BGD): 更新每一参数时, <strong>使用所有样本</strong>来进行更新(优点: 全局优化并行; 缺点: 训练过程慢)    
- 小批量梯度下降(MBGD): 更新每一参数时, <strong>使用一部分样本</strong>来进行更新(既能保持)    

### 机器学习的分类    
机器学习    
- 无监督学习   
	- 聚类    
		- K均值聚类   
		- 其他聚类   
	- 降维   
		- PCA   
		- 主题模型  
- 有监督学习  
	- 回归  
		- 线性回归  
		- LOgistic回归  
	- 分类  
		- K-近邻   
		- 朴素贝叶斯   
		- 决策树  
		- 支持向量机  
		- 集成学习  

#### 无监督学习(最典型的算法: 聚类)  
- 使用未被标记的训练样本  
- 根据外部数据的统计规律来调节系统参数, 使输出能反应数据的某种特性   
- 无标签数据$\rightarrow$无监督学习模型$\rightarrow$输出    

#### 有监督学习    
- 使用有标记的训练样本   



### K-means聚类   
- 从<strong>无标签数据</strong>中组织数据的结构, 从而对样本进行分组, 是无监督学习的一种常用聚类方法.  
- K-means聚类算法流程可以分为四个步骤, 需要注意几点:  
	- 距离度量: 欧氏距离, 余弦相似度等    
	- 质心计算: 样本均值   
- K的取值是不确定的
